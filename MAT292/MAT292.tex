\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\linespread{1.1}


\begin{document}

\title{MAT292 \\ Ordinary Differential Equations}
\author{Michael Boyadjian}
\maketitle
\pagebreak

\tableofcontents

\pagebreak
\section{Introduction}
\subsection{Phase Lines and Direction Fields}
A first order \textbf{autonomous} differential equation is in the form 
$$ \frac{dy}{dt} = f(y)$$ 
This means that the independent variable, $t$, does not appear on the right side of the equation. Other examples include:
$$x' = \sin x \quad \quad \quad p'  = rp\left(1-\frac{p}{k}\right)$$
The first step n qualitative analysis is to find the constant solutions, where $f(y) = c$. In these cases, $\frac{dy}{dt} = 0$ and is considered an \textbf{equilibrium} solution. These are also referred to as critical points, stationary points, and fixed points.
\\ \\
We can graph the $f(y)$ vs $y$ in what is known as the  \textbf{phase line} or \textbf{one dimensional phase portrait} to see the behaviour of $y'$. Seeing when $y'$ is increasing or decreasing, we can use the information to then sketch a \textbf{integral curve} where we can sample plots of $y$ vs $t$
\subsection{Definitions, Classification, and Terminology}
\begin{itemize}
\item \textbf{Differential Equation:} An equation that contains de derivatives of one or more unknown functions with respect to one or more independent variables is said to be a differential  equation.
\item \textbf{Ordinary Differential Equation (ODE):} The unknown function (or functions) depends on a single variable and its derivatives are ordinary derivatives.
\item \textbf{Partial Differential Equation (ODE):} The unknown function (or functions) depends on more than one independent variable and partial derivatives appear in the equation
\item \textbf{Order:} The order of the highest derivative, ordinary or partial, that appears in the equation
\item \textbf{Linear:} An ODE is linear considered linear if it can be written in the form $$a_0(t)y^n + a_1(t)y^{n-1} + \cdots + a_n(t)y = g(t)$$ where the functions $a_0$, $a_1$, $\cdots$, $a_n$ depend at most on the independent variable $t$.
\begin{itemize}
\item \textbf{Homogeneous:} $g(t) = 0$ for all $t$
\item \textbf{Non-Homogeneous:} $g(t) = f$
\end{itemize}
\item \textbf{Non-Linear:} An ODE is non-linear if it cannot take the form of a linear equation
\end{itemize}

\pagebreak
\section{1st Order Differential Equations}
\subsection{Separable Equations}
Consider a first order linear differential equation of form $\frac{dy}{dx} = f(x,y)$. If the right side of this equation, $f(x,y)$ can be broken down to $p(x)q(y)$, it is classified as a separable equation.
$$\frac{dy}{dx} = f(x,y) = p(x)q(y)$$
This is easily solved by rearranging the equation to isolate the $x$ and $y$ variables and then integrating to find a general solution.
\subsection{Method of Integrating Factors}
A differential equation is said to be a \textbf{first order linear equation} if it can be written in the form
$$\frac{dy}{dx} + p(t)y = g(t) \quad \quad \rightarrow \quad \quad y' +py = g$$
\underline{The solution can be found by going through the following steps:}
\begin{enumerate}
\item Put the equation in standard form $y' +py =g$
\item Calculate the integrating factor $\mu (t) = e^{\int p(t)dt}$
\item Multiply the equation by $\mu (t)$ and write in the form $[\mu (t)y]' = \mu (t)g(t)$
\item Integrate this equation to obtain $ \mu (t)y = \int \mu(t)g(t)dt + C$
\item Solve for $y$
\end{enumerate}
\subsection{Differences Between Linear and Non-Linear Equations}
Before attempting a problem it is important to know if there is even a solution t it. This is where we can discuss the \textbf{existence and uniqueness of solutions}. There are theorems which cover this for both linear and non-linear equations.\\ \\
\textbf{For Linear Equations (Theorem 2.4.1):}
If the functions $p$ and $g$ are continuous on an open interval $I = (\alpha, \beta)$ containing the point $t = t_0$, there exists a unique function $y = \Phi(t)$ that satisfies the differential equation
$$ y' + p(t)y = g(t)$$
for each $t$ in $I$, and that also satisfies the initial condition,
$$y(t_0) = y_0$$
where $y_0$ is an arbitrary prescribed value. \\\\
A quick summary of the properties of linear differential equations:
\begin{enumerate}
\item Assuming the coefficients are continuous, there is a general solution, containing an arbitrary constant, that includes all solutions
\item There is an expression for the solution
\item The possible points of discontinuity or singularities can be identified by finding the points of discontinuity of the coefficients
\end{enumerate}
\textbf{For Non-Linear Equations (Theorem 2.4.2):} Let the functions $f$ and $\frac{\partial f}{\partial y}$ be continuous in some rectangle $ \alpha < t < \beta $, $\gamma < t < \delta$ containing the point $(t_0, y_0)$. Then in some interval $t_0 - h < t < t_0 + h$ contained in $\alpha < t < \beta$, there is a unique solution $y=\Phi(t)$ of the initial value problem,
$$y'(t) = f(t, y) \quad \quad \quad y(t_0) = y_0$$
Looking at the properties of linear differential equations, none of them are true in general, for non-linear equations. It comes down to a case-by-case basis
\subsection{Autonomous Equations and Population Dynamics}
As previously mentioned, an autonomous equation takes the form
$$ \frac{dy}{dt} = f(y)$$
\subsubsection{Exponential Growth}
The simplest analysis of population occurs with the assumption that the rate of change of the population is proportional to the current value of the population. This is modelled as
$$ \frac{dP}{dt} = rP$$ where $r$ is the rate of growth or decline depending on its sign. Given the initial condition $P(0) = P_0$, the solution is given as 
$$ P(t) = P_0e^{rt}$$
\subsubsection{Logistic Growth}
We replace the constant $r$ with a function $h?(P)$ from the previous equation to obtain the modified one 
$$ P(t) = h(P)\cdot P$$
We choose $h(P) = r-aP$ where $a$ is also a positive constant. The equation now becomes
$$P(t) = (r-aP)P = r\left(1-\frac{P}{K}\right) P$$
where $K=\frac{r}{a}$ and is known as the intrinsic growth rate

\subsection{Exact Equations}
Let the functions $M$, $N$, $M_y$, and $N_x$, where subscripts denote partial derivatives, be continuous in the rectangular  region $R$: $\alpha < x < \beta$, $\gamma < y < \delta$. Then, $$M(x,y)+N(x,y)y' = 0$$ , is an exact differential equation in $R$ if and only if $$M_y(x,y)=N_x(x,y)$$ at each point of $R$. That is, there exists a function $\psi$ satisfying $$\psi_ x(x,y)=M(x,y) \quad \quad \psi_y(x,y)=N(x,y)$$ 

\pagebreak

\section{Systems of 2 1st Order Equations}
\subsection{Systems of Two First Order Linear Differential Equations}
The general system of two first order linear differential equations is
$$\begin{bmatrix}
dx/dt \\
dy/dt
\end{bmatrix}
= \begin{bmatrix}
P_{11}(t)x + P_{12}(t)y + g_1(t) \\
P_{21}(t)x + P_{22}(t)y + g_2(t) \\
\end{bmatrix}$$
We can also rewrite this in vector form $\frac{d \vec{X}}{dt} = \vec{P}(t)\vec{X}+g(t)$:
$$\begin{bmatrix}
dx/dt \\
dy/dt
\end{bmatrix}
= \begin{bmatrix}
P_{11}(t) + P_{12}(t)\\
P_{21}(t) + P_{22}(t)\\
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
+
\begin{bmatrix}
g_1(t) \\
g_2(t)
\end{bmatrix}
$$
Since we are often looking for a specific solution to a problem, we need to specify the initial conditions.
$$ x(t_0) = x_0 \quad \quad \text{and} \quad \quad y(t_0) = y_0$$
\subsection{Homogeneous Linear Systems with Constant Coefficients}
\subsubsection{Reducing $\vec{x}' = A\vec{x} + \vec{b}$ to $\vec{x}' = A\vec{x}$}
If the coefficient matrix $A$ is invertible, we know that the only critical or equilibrium point of $\vec{x'} = A\vec{x} + \vec{b}$ is $\vec{x}_{eq} = -A^{-1}\vec{b}$. In these cases, it is convenient to shift the origin of the phase plane to the equilibrium solution using
$$\vec{x} = \vec{x}_{eq} + \vec{\tilde{x}}$$
We can see that $\vec{\tilde{x}} = \vec{x} - \vec{x}_{eq}$ represents the difference between $\vec{x}$ and the equilibrium state, $\vec{x}_{eq}$. Thus, we get that
$$\frac{d\vec{\tilde{x}}}{dt} = \vec{\tilde{x}}' = A\vec{\tilde{x}}$$
\subsubsection{Principle of Superposition}
If $x_1(t)$ and $x_2(t)$ are solutions to  $$\vec{x}'(t) = A\vec{x}(t)$$ then the expression $$\vec{x}(t) = C\vec{x}_1(t) + D\vec{x}_2(t)$$ where $C$ and $D$ are arbitrary constants, is also a solution
\subsubsection{Wronskian and Linear Independence}
Suppose that $$x_1(t) = \begin{pmatrix}
x_{11}(t) \\
x_{21}(t)
\end{pmatrix} \quad \quad \text{and} \quad \quad x_2(t) = \begin{pmatrix}
x_{12}(t) \\
x_{22}(t)
\end{pmatrix}$$ are solutions on some interval, $I$. Then $\vec{x_1}$ and $\vec{x_2}$ are linearly independent if and only if 
$$W[x_1, x_2](t) = \begin{vmatrix}
x_{11}(t) & x_{12}(t) \\
x_{21}(t) & x_{22}(t)
\end{vmatrix} \neq 0 \quad \text{  for all $t$ in $I$}$$
\subsubsection{The Eigenvalue Method for Solving $\vec{x}' = A\vec{x}$ }
We will consider a system that will usually be written as
$$\frac{d\vec{x}}{dt} = A\vec{x}$$ where the elements of matrix $A$ are real and the elements of vector $\vec{x}$ are to be determined. \\ \\
We know that $$A\vec{v} = \lambda \vec{v} \quad \quad \text{and} \quad \quad (A-\lambda I)\vec{v} = 0$$ and that finding the eigenvalues and eigenvectors of this equation will yield our solution $$ \vec{x} = e^{\lambda t} \vec{v}$$
The eigenvalues $\lambda_1$ and $\lambda_2$ are found by solving the characteristic equation
$$ det(A - \lambda I) = 
\begin{vmatrix}
a_{11} - \lambda & a_{12} \\
a_{21} & a_{22} - \lambda
\end{vmatrix} = (a_{11} - \lambda)(a_{22} - \lambda) - (a_{12})(a_{21})$$
After finding each eigenvalue, they can be used to find the corresponding eigenvectors $\vec{v_1}$ and $\vec{v_2}$. Assuming the eigenvalues, $\lambda_1$ and $\lambda_2$ are real and different, the two solutions can be written as 
$$\vec{x_1}(t) = e^{\lambda_1 t}\vec{v_1} \quad \quad \text{and} \quad \quad \vec{x_2}(t) = e^{\lambda_2 t}\vec{v_2}$$ Since the Wronskian of vectors $\vec{v_1}$ and $\vec{v_2}$ is non-zero, the general solution is
$$ \vec{x}(t) = Ce^{\lambda_1 t}\vec{v_1} + De^{\lambda_2 t}\vec{v_2}$$
\subsection{Complex Eigenvalues}
Let's consider again the system $\vec{x'} = A\vec{x}$. The eigenvalues of this system are the solution to $\det{(A-\lambda I)} = \vec{0}$ which is the solution to $(A-\lambda I)\vec{v} = \vec{0}$. Since $A$ is real valued, any complex eigenvalues must occur in conjugate pairs \textit{(i.e. if $\lambda_1 = \mu + iv$ is an eigenvalue, then so is $\lambda_2 = \mu - iv$)}.
\\ \\ Considering this case, if $v_1$ is the corresponding eigenvector to $\lambda_1$, then it will satisfy
$$ (A-\lambda_1 I)\vec{v_1} = \vec{0}$$ Similarly, by taking the conjugate, $$ (A-\bar{\lambda_1} I)\bar{\vec{v_1}} = \vec{0}$$
Using these two eigenvectors, we determine the solution to the system as 
$$ x_1(t)= e^{(\mu +iv)t}\vec{v_1} \quad \quad \text{and} \quad \quad x_2(t)= e^{(\mu -iv)t}\bar{\vec{v_1}}$$
Using Euler's identity, we can then separate these equations into their real and imaginary parts
$$e^{ivt} = \cos{vt} + i\sin{vt}$$
Let's set $\vec{v_1} = a + ib$. We now have
$$\vec{x_1}(t) = (a+ib)e^{\mu t}(\cos{vt} + i\sin{vt})$$
Now by separating the parts, we obtain
$$u(t) = e^{\mu t}(a\cos{vt} - b\sin{vt}) \quad \quad \text{and} \quad \quad w(t) = e^{\mu t}(a\cos{vt} + b\sin{vt})$$
We see that $$\vec{x}_2(t) = u(t) - iw(t)$$ meaning that $\vec{x_1}(t)$ and $\vec{x_2}(t)$ are complex conjugates of each other. \\ \\ \underline{The following is the procedure for finding a solution when $A$ has complex eigenvalues:}
\begin{enumerate}
\item Identify the complex conjugate eigenvalues $\lambda = \mu \pm iv$ 
\item Determine the eigenvector corresponding to $\lambda_1 = \mu + iv$
\item Express the eigenvector $\vec{v}$ in the form $\vec{v} = a+ib$
\item Write the general solution by separating into real and imaginary parts $$\vec{x_1}(t) = u(t) + w(t) = e^{\mu t}(a\cos{vt} - b\sin{vt})+ ie^{\mu t}(a\cos{vt} + b\sin{vt})$$
\item The general solution is then $$\vec{x_1}(t) = Cu(t) + Dw(t)$$

\end{enumerate}

\pagebreak

\section{Numerical Methods}
\subsection{Euler's Method}
Given the initial value problem
$$ \frac{dy}{dt} = f(t,y) \quad \quad \quad y(t_o) = y_o $$ then the approximation of $y= \phi (t)$ is 
$$ y_{n+1} = y_n + f(t_n, y_n)(t_{n+1} - t_n) $$
If a uniform step size $h$ is used , then $t_{n+1} - t_n = h$, for all $n$, and the above equation simplifies to
$$y_{n+1} = y_n + hf(t_n, y_n)$$

\subsection{Improved Euler Method}
Given the initial value problem
$$ \frac{dy}{dt} = f(t,y) \quad \quad \quad y(t_o) = y_o $$ then the approximation of $y= \phi (t)$ is 
$$ y_{n+1} = y_n + \frac{h}{2} \left( f(t_n, y_n)+ f[t_n + h, y_n + hf(t_n, y_n)] \right) $$
If $f(t,y) = f(t)$, then the above equation simplifies to
$$y_{n+1} = y_n + \frac{h}{2} [f(t_n) + f(t_n + h)] $$
\subsection{Runge-Kutta Method}
Given the initial value problem
$$ \frac{dy}{dt} = f(t,y) \quad \quad \quad y(t_o) = y_o $$ then the approximation of $y= \phi (t)$ is
$$ y_{n+1} = y_n + h \left( \frac{k_{n1} + 2k_{n2} + 2k_{n3} + k_{n4}}{6}\right)$$
where
$$ k_{n1} = $$ 
$$ k_{n2} = $$
$$ k_{n3} = $$
$$ k_{n4} = $$
If $f(t,y) = f(t)$, then $$y_{n+1} = y_n + \frac{h}{6}\left[f(t_n) + 4f\left(t_n + \frac{h}{2}\right) + f(t_n + h)\right]$$

\pagebreak

\section{Systems of First Order Linear Equations}
\subsection{Basic Theory}
\subsubsection{Existence and Uniqueness for First Order Linear Systems}
If $P(t)$ and $g(t)$ are continuous on an open interval $U = (\alpha, \beta)$, then there exists a unique solution $X = \Phi (t)$ of the initial value problem 
$$ x' = P(t)x + g(t) \quad \quad	x(t_o) = x_o$$
where $t_o$ is any point in $I$ and $x_o$ is any constant with $n$ components. Moreover the solution exists throughout the interval $I$.
\subsubsection{Principle of Superposition}
If $x_1, x_2, ..., x_k$ are solutions to the homogeneous linear system $x' = P(t)x$ on the interval $I = (\alpha, \beta)$, then the linear combination
$$c_1x_1 + c_2x_2 +\cdots +c_kx_k$$ is also a solution on $I$.
\subsection{Fundamental Matrices}
\subsubsection{The Fundamental Matrix}
If a fundamental set of solutions to an equation given by $\vec{x}' = P(t)\vec{x}$ is $\vec{x_1}(t), ..., \vec{x_n}$ then we can define what is called the \textbf{fundamental matrix}:
$$X(t) = [\vec{x_1}(t), \vec{x_2}(t), ..., \vec{x_n}(t)]$$
The columns of the fundamental matrix are made up of the linearly independent solutions of the system. \\ \\ 
Given an initial value problem whose general solution is $$ \vec{x} = c_1\vec{x_1}(t) + ... + c_n\vec{x_n}(t)$$, we can write this solution in terms of the fundamental matrix
$$\vec{x} = X(t)\vec{c} = X(t)X^{-1}(t_0)x_0$$
\subsubsection{The Matrix Exponential Function $e^{At}$}
Let $A$ be an $n \times n$ constant matrix. The matrix exponential function is defined to be 
$$ e^{At} = I_n + At + \frac{1}{2!}A^2t^2 + \frac{1}{3!}A^3t^3 + \cdots = \sum_{k=0}^{\infty} A^k \frac{t^k}{k!}$$
This expression can be simplified to $$ e^{At} = \Phi(t)$$ which tells us that the solution to the initial value problem $x'=Ax$, $x(0) = x_0$, is $x = e^{At}x_0$ The following properties hold true for the matrix exponential function:
\begin{enumerate}
\item $e^{A(t+\tau} = e^{At}e^{A\tau}$
\item $Ae^{At} = e^{At}A$ 
\item $(e^{At})^{-1} = e^{-At}$
\item $e^{(A+B)t} = e^{At}e^{Bt}$, if $AB = BA$
\end{enumerate}
In order to construct the matrix exponential function, follow these steps:
\begin{enumerate}
\item Generate the fundamental matrix $X(t)$
\item Find $X^{-1}(0)$
\item Compute $e^{At} = X(t)X^{-1}(0)$ 
\end{enumerate}

\pagebreak


\section{Second Order Linear Equations}
\subsection{Definitions}
A second order differential equation involving the independent variable $t$, and an unknown function or dependent variable $y = y(t)$ along with its first and second derivatives. This equation is in the form
$$ y'' = f(t, y, y')$$
An initial value problem for a second order equation on an interval $I$ includes two initial conditions
$$ y(t_o) = y_o \quad \quad y'(t_o) = y_1$$
A second order differential equation is said to be linear if it can be written in the standard form
$$y'' + p(t)y' +q(t)y = g(t)$$
where the coefficient of $y''$ is equal to 1. If $g(t)=0$ for all $t$, the equation is said to be \textbf{homogeneous}. It is otherwise \textbf{non-homogeneous}.
\subsection{Theory of Second Order Linear Homogeneous Equations}
\subsubsection{Existence and Uniqueness of Solutions}
Let $p(t)$, $q(t)$, and $g(t)$ be continuous on an open interval $I$, let $t_0$ be any point in $I$, and let $y_0$ and $y'_0$ be given numbers. Then there exists a unique solution $y= \phi(t)$ of the differential equation  $$y'' + p(t)y' +q(t)y = g(t)$$
that satisfies the given initial conditions. The solution exists throughout the interval $I$.\\ \\
In order to find the intervals of existence, simply put the differential equation in standard form and find the points of discontinuity for $p(t)$, $q(t)$, and $g(t)$. Then deduce the correct intervals where the solution will exist.
\subsubsection{Linearity Operators and Superposition Principle}
Let $L[y] = y''+py'+qy$, where $p$ and $q$ are continuous functions on an interval $I$. If $y_1$ and $y_2$ are any twice differentiable functions on $I$ and $c_1$ and $c_2$ are any constants, then 
$$L[c_1y_1 + c_2y_2] = c_1L[y_1] + c_2L[y_2]$$ 
$L[y]$ is called the linearity of the differential operator and the above expression is referred to as the principle of superposition for linear homogeneous equations. \\
\\
Let $L[y] = y'' + py' +qy$, where $p$ and $q$ are continuous functions on an interval $I$. If $y_1$ and $y_2$ are solutions of $L[y] = 0$, then the linear combination
$$ y = c_1y_1(t) + c_2y_2(t)$$
is also a solution for any values of the constants $c_1$ and $c_2$

\subsubsection{Wronskians and Fundamental Sets of Solutions} 
As previously mentioned, the Wronskian is given as 
$$W[x_1, x_2](t) = \begin{vmatrix}
x_{11}(t)&x_{12}(t) \\
x_{21}(t)&x_{22}(t) 
\end{vmatrix}$$
Suppose that $x_1$ and $x_2$ are two solutions of $x'=P(t)x$ and their Wronskian is not zero on $I$. Then $x_1$ and $x_2$ form a fundamental set of solutions and the general solution is given by
$$x = c_1x_1(t) + c_2x_2(t)$$ where $c_1$ and $c_2$ are arbitrary constants. If an initial condition is given, this can be used to determine the constants uniquely.
\\ \\
This same theorem applies to the second order differential equation $$y'' + p(t)y' +q(t)y = g(t)$$
\subsubsection{Abel's Theorem}
The Wronskian of two solutions of the system $x' = P(t)(x)$ is given by
$$W(t) = c \left[ \exp \int tr (P)(t) dt\right] = c \exp \int [p_{11}(t) + p_{22}(t)]dt$$ For a second order equation of the form $y'' + p(t)y' +q(t)y = g(t)$, the Wronskian is given by  
$$W(t) = c  \exp\left[ - \int(p)(t) dt\right]
$$ where $c$ is a constant that depends on the pair of solutions. An important result may also be that the first derivative of the Wronskian is the following:
$$ W' = (p_{11} + p_{22})(x_{11}x_{22} - x_{12}x_{21})$$


\subsection{Linear Homogeneous Equations with Constant Coefficients}
We'd like to find the set of solutions to the following equation with constant coefficients:
$$ay'' + by'+cy =0$$
\subsection{Mechanical and Electrical Vibrations}
\subsubsection{Undamped Free Vibrations}
Recall that the equation of motion for a damped spring-mass system with external forcing is $$my'' +\gamma y' + ky = F(t)$$
With no damping or forcing, this equation reduces to $$y'' + \omega_0^2 y = 0$$ The general solution for this problem is $$ y = A\cos \omega_0 t + B \sin \omega_0 t $$
\subsubsection{Damped Free Vibrations}
Re-introducing damping, we obtain the equation of motion $$my'' +\gamma y' + ky = 0$$ There are three cases to consider, which is based on the discriminant $\gamma^2 - 4km$
\begin{enumerate}
\item \textbf{Underdamped Harmonic Motion:} In this case $\gamma^2 - 4km < 0$. The roots are complex and the general solution is 
$$ y = e^{-\gamma t/2m}(A\cos vt + B\sin vt)$$
\item \textbf{Critically Damped Harmonic Motion:} In this case $\gamma^2 - 4km = 0$. The roots are repeated and the general solution is 
$$ y = (A+Bt)e^{-\gamma t/2m}$$
\item \textbf{Over Damped Harmonic Motion:} In this case $\gamma^2 - 4km > 0$. The roots are real, distinct, and negative. The general solution is 
$$ y = Ae^{\lambda_1 t} + Be^{\lambda_2 t}$$

\end{enumerate}
\subsection{Non-Homogeneous Equations}
Let's consider the non-homogeneous equation 
$$y'' +p(t)y'+q(t)y=g(t)$$
The general solution to this problem can be written as $$y = \phi(t) = Ay_1(t)+By_2(t)+Y(t)$$
where $y_1$ and $y_2$ form a fundamental set of solutions corresponding to the homogeneous equation (where $g(t) = 0$) and Y is some specific solution to the non-homogeneous equation.  \\ \\ There are 3 steps to solve the non-homogeneous equation:
\begin{enumerate}
\item Find the general solution $c_1y_1(t) +c_2y_2(t)$ of the corresponding homogeneous equation. This is called the \textbf{complimentary solution} ($y_c(t)$).
\item Find some single solution $Y(t)$ of the non-homogeneous equation. This is called the \textbf{particular solution} ($y_p(t)$).
\item Combine the two solutions in the form $y(t) = y_c(t) + y_p(t)$
\end{enumerate}
In order to find the particular solution, we can use the method of undetermined coefficients. To use this method, we must come up with a guess to the function $y_p(t)$ that may satisfy the differential equation. The following summarizes the guess functions depending on $g(t)$.
\begin{itemize}
\item $a e^{\beta t} \quad \rightarrow \quad Ae^{\beta t}$ 
\item $a \cos {\beta t} \quad \rightarrow \quad A \cos {\beta t} + B \sin {\beta t}$ 
\item $b \sin {\beta t} \quad \rightarrow \quad A \cos {\beta t} + B \sin {\beta t}$ 
\item $a \cos {\beta t} + b \sin {\beta t} \quad \rightarrow \quad A \cos {\beta t} + B \sin {\beta t}$ 
\item $n^{th}$ degree polynomial $\quad \rightarrow \quad A_nt^n + A_{n-1}t^{n-1} + \cdots A_1t + A_0$

\end{itemize}
 
\end{document}