\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{cancel}
\usepackage{graphicx}
\linespread{1.1}


\begin{document}

\title{ECE286 \\ Probability and Statistics}
\author{Michael Boyadjian}
\maketitle
\pagebreak

\tableofcontents

\pagebreak

\bigskip
\bigskip
\bigskip


\section{Introduction to Statistics and Data Analysis}

\subsection{Definitions}
\begin{itemize}
\item \textbf{Probability:} Mathematical theory that models and analyses uncertainty based on 3 fundamental axioms. 
\item \textbf{Statistics:} Empirical; based on data and measurements; we infer characteristics of a phenomenon
\end{itemize}

\subsection{Set Theory}
A set is a set of objects (or elements) represented using capital letters $A$, $B$, $C$, $S$
\begin{itemize}
\item \textbf{Universal Set:} Set of all elements or objects of interest
\item \textbf{Empty Set:} Set without any elements, denoted $\varnothing$
\item \textbf{Compliment of a Set:} $A' = \{x | x \notin A\} $
\item \textbf{Union of Two Sets:} $ A \cup B = \{x | x \in A \text{ or } x \in B\}$
\item \textbf{Intersection of Two Sets:} $ A \cap B = \{x | x \in A \text{ and } x \in B\}$
\item \textbf{Disjoint Sets:}  Also called mutually exclusive; $A$ and $B$ are disjoint iff  $A \cap B = \varnothing$
\item \textbf{Subsets:} Denoted $A \subset B$; $A$ is a subset of $B$ iff $x \in A \implies x \in B$
\end{itemize}

\subsection{Counting}
There are three different methods of counting
\begin{enumerate}
\item \textbf{With Replacement, With Ordering}
\begin{itemize}
\item Given $n$ distinct objects pick one, note down its kind, and put it back
\item Repeat $k$ times
\item Produces $n^k$ possibilities
\item \textit{ex.} Number of 8-character passwords $= 62^8$
\end{itemize}
\item \textbf{Without Replacement, With Ordering}
\begin{itemize}
\item Same as previous, but no replacement
\item Produces $\frac{n!}{(n-k)!}$ possibilities
\item Describes number of \textbf{permutations}, denoted $_nP_k$
\item \textit{ex.} Number of 8-character passwords without repeating characters $= \frac{62!}{54!}$
\end{itemize}
\bigskip
\item \textbf{Without Replacement, Without Ordering}
\begin{itemize}
\item Ordering does not matter, two sequences with same objects count as the same
\item Produces $\frac{n!}{k!(n-k)!}$ possibilities
\item Describes number of \textbf{combinations}, denoted $_nC_k = {n \choose k}$
\item \textit{ex.} Number of ways to pick 5 good chips and 3 bad chips out of 90 good and 10 defective $= {90 \choose 5}{ 10 \choose 3}$
\end{itemize}
\end{enumerate}

\pagebreak

\section{Probability}
Probability theory rests on the notion of a random experiment. We don't know what the outcome will be until we actually run the experiment. There are three kinds of experiments:
\begin{itemize}
\item Designed
\item Observational
\item Retrospective
\end{itemize}
An experiment has a procedure and a measurement and always produces one outcome

\subsection{Sample Spaces}
The sample space describes the set of all possible outcomes  $(S)$. There are three different kinds of sample spaces
\begin{itemize}
\item Finite $\rightarrow$ discrete
\item Countably Infinite $\rightarrow$ discrete
\item Continuous
\end{itemize}

\subsection{Events}
An event is the set of outcomes we are interested in. This must be a subset of the sample space. The \textbf{event class} is the set of all subsets of the sample space. Probabilities will be assigned to the events in $\varepsilon$

\subsection{Relative Frequency}
We want to assign probabilities consistent with the idea of relative frequency, which is defined as 
$$\lim_{n \to \infty} \frac{n_A}{n} = p_A$$
This is only useful to model the likelihood of events occurring, so probabilities are assigned using probability axioms

\subsection{Axioms of Probability}
\begin{enumerate}
\item For any event $A$, $P(A) \geq 0$
\item $P(S) = 1$
\item For any two events $A$ and $B$ such that $P(A \cap B) = \varnothing$, then $ P(A\cup B) = P(A) + P(B)$
\end{enumerate}

\subsection{Properties}
\begin{enumerate}
\item $P(A') = 1 - P(A)$
\item $P(A) \leq 1$
\item $P(\varnothing) = 0$
\item For any events $A$ and $B$: $P(A \cup B) = P(A) + P (B) - P(A \cap B)$
\end{enumerate}

\subsection{Conditional Probability}
Conditional probability describes the probability of some event $B$ occurring given an event $A$ has occurred. This is denoted as $P(B|A)$, which is read as \textit{"the probability of $B$ given $A$"}. \\ \\ A conditional probability relative to a subspace $A$ of $S$ may also be calculated directly from the probabilities assigned to the elements of the original sample space $S$
$$ P(B|A) = \frac{P(A \cap B)}{P(A)}$$

\subsubsection{Independent Events}
Two events are said to be independent if and only if 
$$P(B|A) = P(B) \quad \quad \text{or} \quad  \quad P(A|B) = P(A)$$
$A$ and $B$ are otherwise dependent

\subsubsection{Product Rule and Multiplicative Rule}
If in an experiment, both events $A$ and $B$ can occur, then 
$$ P(A \cap B) = P(A)P(B|A)$$
We can use this to find another expression for the independence of events. Two events $A$ and $B$ are independent if and only if 
$$ P(A \cap B) = P(A)P(B)$$
This multiplicative rule can be extended to more than just two-event situations. If, in an experiment, the events $A_1$, $A_2$, ... $A_k$ occur, then
$$ P(A_1 \cap A_2 \cap \cdots \cap A_k) = P(A_1)P(A_2|A_1)P(A_3|A_1 \cap A_2) \cdots P(A_k| A_1 \cap A_2 \cap \cdots A_{k-1})$$ If these events are independent, then 
$$ P(A_1 \cap A_2 \cap \cdots \cap A_k) = P(A_1)P(A_2)P(A_3) \cdots P(A_k)$$

\subsection{Bayes' Rule}
Bayes' Rule tells us the following. If the events $B_1$ , $B_2$ , . . . , $B_k$ constitute a partition of the sample space $S$ such that $P(B_i) \neq 0$ for$ i = 1$,$2$,$...$,$k$, then for any event $A$ in $S$ such that $P(A) \neq 0$,
$$ P(B_r | A) = \frac{P(B_r \cap A)}{\sum_{i=1}^{k} P(B_i \cap A) } = \frac{P(B_r) P(A | B_r)}{\sum_{i=1}^{k} P(B_i)P(A|B_i) }$$

\pagebreak

\section{Random Variables and Probability Distributions}

\subsection{Concept of a Random Variable}
A random variable is a function that associates a real number with each element in the sample space. This allows us to do math on a random experiment and map outcomes to the real line. We can classify a sample space of random variables in two ways:
\begin{itemize}
\item \textbf{Discrete Sample Space:}  Contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers
\item \textbf{Continuous Sample Space:} Contains an infinite number of possibilities equal to the number of points on a line segment
\end{itemize}

\subsection{Discrete Probability Distributions}
A discrete random variable assumes each of its values with a certain probability. The set of ordered pairs $(x, f (x))$ is a \textbf{probability function}, \textbf{probability mass function}, or \textbf{probability distribution} of the discrete random variable $X$ if, for each possible outcome $x$
\begin{enumerate}
\item $f(x) \geq 0$
\item $\sum_x  f(x) = 1$
\item $P(X=x) = f(x)$
\end{enumerate}
In cases where we want to determine the probability that the observed value $X$ is less than or equal to some real $x$, we can use what is called the \textbf{cumulative distribution function}. $F(x)$ of a discrete random variable $X$ with probability distribution $f(x)$ is 
$$ F(x) = P(X \leq x) = \sum_{t \leq x} f(t) \quad \quad \text{for } -\infty < x < \infty $$

\subsection{Continuous Probability Distributions}
A continuous random variable has a probability of $0$ of assuming exactly any of its values, so we are unable to give its probability distribution in tabular form. In dealing with continuous distributions, note the following:
$$P(a<x\leq b) = P(a<x<b) + P(x=b) = P(a<x<b)$$
This tells us that we do not need to include an endpoint of the interval if $X$ is continuous. We can define a probability density function for a continuous random variable $X$. The function $f(x)$ is a \textbf{probability density function} for the continuous random variable $X$, defined over the set of real numbers, if
\begin{enumerate}
\item $f(x) \geq 0$, for all $x \in R$
\item $\int \limits_{-\infty}^{\infty} f(x) dx = 1$
\item $P(a<x<b) = \int \limits_{a}^{b} f(x) dx $
\end{enumerate}
Similar to discrete distributions, we can also define a probability distribution function to determine the probability that the observed value $X$ is less than or equal to some real $x$. The \textbf{cumulative distribution function} $F(x)$ of a continuous random variable $X$ with density $f(x)$ is 
$$ F(x) = P(X \leq x) = \int \limits_{-\infty}^{x} f(t) dt\quad \quad \text{for } -\infty < x < \infty $$

\subsection{Joint Probability Distributions}
\subsubsection{Discrete RVs}
If $X$ and $Y$ are two \underline{discrete} random variables, the probability distribution for their simultaneous occurrence can be represented by $f(x,y)$ for any pair of values $(x, y)$ within the range of the random variables $X$ and $Y$ . It is customary to refer to this function as the \textbf{joint probability distribution} of $X$ and $Y$. 
\\ \\
$f(x,y)$ is a \textbf{joint probability distribution} or \textbf{probability mass function} of the discrete random variables $X$ and $Y$ if
\begin{enumerate}
\item $f(x,y) \geq 0$ for all $(x,y)$
\item $\sum_x \sum_y f(x,y) = 1$
\item $P(X=x, Y=y) = f(x,y)$
\end{enumerate}
For any region $A$ in the $xy$ plane, $P[(X, Y) \in A] = \sum \sum_A f(x,y)$
\subsubsection{Continuous RVs}
When $X$ and $Y$ are \underline{continuous} random variables, the \textbf{joint density function} $f(x,y)$ is a surface lying above the $xy$ plane, and $P[(X,Y) \in A]$, where $A$ is any region in the $xy$ plane, is equal to the volume of the right cylinder bounded by the base $A$ and the surface.
\\ \\ 
The function $f(x,y)$ is a \textbf{joint density function} of the continuous random variables $X$ and $Y$ if 
\begin{enumerate}
\item $f(x,y) \geq 0$ for all $(x,y)$
\item $\int \limits_{-\infty}^{\infty} \int \limits_{-\infty}^{\infty} f(x,y) dx dy = 1$
\item $P[(X, Y) \in A] = \int \int _A f(x,y) dx dy$ for any region $A$ in the $xy$ plane
\end{enumerate}

\pagebreak
\subsubsection{Marginal Distributions}
Given the joint probability distribution $f (x, y)$ of the discrete random variables $X$ and $Y$ , the probability distribution $g(x)$ of $X$ alone is obtained by summing $f (x, y)$ over the values of $Y$ . Similarly, the probability distribution $h(y)$ of $Y$ alone is obtained by summing $f(x,y)$ over the values of $X$. We define $g(x)$ and $h(y)$ to be the \textbf{marginal distributions} of $X$ and $Y$ , respectively. When $X$ and $Y$ are continuous random variables, summations are replaced by integrals. 
\\ \\
The marginal distributions ox $X$ alone and $Y$ alone are
$$ g(x) = \sum \limits _y f(x,y) \quad \quad \text{and} \quad \quad  h(y) = \sum \limits _x f(x,y)$$
$$ g(x) = \int \limits_{-\infty}^{\infty} f(x,y) \quad \quad \text{and} \quad \quad  h(y) = \int \limits_{-\infty}^{\infty} f(x,y)$$
for the discrete and continuous cases respectively

\section{Mathematical Expectation}

\subsection{Mean of a Random Variable}
\subsection{Variance and Covariance of Random Variables}
\subsection{Means and Variance of Linear Combinations of Random Variables}

\pagebreak
\section{Discrete Probability Distributions}
\subsection{Binomial Distribution}
\subsection{Hypergeometric Distribution}
\subsection{Negative Binomial and Geometric Distributions}
\subsection{Poisson Distribution}

\pagebreak

\section{Continuous Probability Distributions}
\subsection{Uniform Distribution}
The continuous uniform distribution is one of the simplest statistical distributions. It has a "flat" density function with uniform probability in an interval [$A$, $B$]. It is  often referred to as a \textbf{"rectangular distribution"} as the density function forms a rectangle with base $B-A$ and a constant height of $\frac{1}{B-A}$. \\ \\
The density function of the continuous uniform random variable $X$ on the interval [$A$, $B$] is
$$ f(x; A, B) =   \left\{
\begin{array}{lll}
      \frac{1}{B-A} & \text{,} & A\leq x\leq B \\
      0 & \text{,} & \text{elsewhere} \\
\end{array} 
\right. $$
The mean and variance of the uniform distribution are given as follows:
$$ \mu = \frac{A+B}{2} \quad \quad \quad \sigma^2 = \frac{(B-A)^2}{12}$$
\subsection{Normal Distribution}
The normal distribution, often called the \textbf{Gaussian distribution}, is the most important distribution in statistics. Its graph is called the \textbf{normal curve} and appears as a bell shape and a continuous variable $X$ having this distribution is called the \textbf{normal random variable}. Thus, the density of the normal random variable $X$, with mean $\mu$ and variance $\sigma^2$, is given over all space as
$$ n(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma} e ^ {- \frac{1}{2\sigma^2} (x-\mu)^2}$$
Some important properties of the normal distribution include the following:
\begin{enumerate}
\item The mode occurs at $x=\mu$
\item The curve is symmetric about a vertical axis through the mean $\mu$
\item The curve has its points of inflection at $x = \mu \pm \sigma$; it is concave downward if $ \mu - \sigma < X < \mu + \sigma $ and is concave upward otherwise.
\item The normal curve approaches the horizontal axis asymptotically as we proceed
in either direction away from the mean
\item The total area under the curve and above the horizontal axis is equal to 1
\end{enumerate}
The mean and variance of the normal distribution are $\mu$ and $\sigma^2$ respectively. Now in order to calculate the probability of $X$ assuming a value between $x_1$ and $x_2$, we would have to integrate for the area under the curve. This could easily be done, however, using what is called the \textbf{standard normal distribution} 
\\ \\ In a standard distribution of a random variable $Z$ the mean is 0 and the variance is 1. We apply the following transformation to obtain the $Z$ variable.
$$ Z  = \frac{X - \mu}{\sigma}$$
Using the values of $Z$ we are thus able to use tables to compute the probabilities
$$ P(x_1 \leq X \leq x_2) = \frac{1}{\sqrt{2\pi} \sigma} \int \limits _{x_1}^{x_2}  e ^ {- \frac{1}{2\sigma^2} (x-\mu)^2} dx = \int \limits _{z_1}^{z_2}  n(z; 0, 1) dz = P(z_1 \leq Z \leq  < z_2)$$
\subsection{Gamma and Exponential Distribution}

\end{document}
